<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>PaperWeekly | wechat-feeds</title><link>http://MzIwMTc4ODE0Mw.favicon.privacyhide.com/favicon.ico</link><description>PaperWeekly是一个推荐、解读、讨论和报道人工智能前沿论文成果的学术平台，致力于让国内外优秀科研工作得到更为广泛的传播和认可。社区：http://paperweek.ly | 微博：@PaperWeekly</description><managingEditor> (hellodword)</managingEditor><pubDate>Wed, 24 Mar 2021 21:37:38 +0800</pubDate><image><url>http://MzIwMTc4ODE0Mw.favicon.privacyhide.com/favicon.ico</url><title>PaperWeekly | wechat-feeds</title><link>http://MzIwMTc4ODE0Mw.favicon.privacyhide.com/favicon.ico</link><width>64</width><height>64</height></image><item><title>登顶GLUE榜单的文心又开课了，一站式教学搞懂信息抽取</title><link>https://mp.weixin.qq.com/s/5QhZO4cozlq7F3RdwyLB_A</link><description></description><content:encoded><![CDATA[登顶GLUE榜单的文心又开课了，一站式教学搞懂信息抽取]]></content:encoded><pubDate>Wed, 24 Mar 2021 19:53:00 +0800</pubDate></item><item><title>详解凸优化、图神经网络、强化学习、贝叶斯方法等四大主题</title><link>https://mp.weixin.qq.com/s/aNqKFk3LX3z_igf17Jw9qA</link><description></description><content:encoded><![CDATA[详解凸优化、图神经网络、强化学习、贝叶斯方法等四大主题]]></content:encoded><pubDate>Wed, 24 Mar 2021 19:53:00 +0800</pubDate></item><item><title>直播 | CVPR 2021论文解读：引入因果结构的解耦表征学习</title><link>https://mp.weixin.qq.com/s/QwIhVE2i5GcmlDEH6CGueA</link><description></description><content:encoded><![CDATA[直播 | CVPR 2021论文解读：引入因果结构的解耦表征学习]]></content:encoded><pubDate>Wed, 24 Mar 2021 19:53:00 +0800</pubDate></item><item><title>只靠开源的时代已经过去，BAT都在这样做！</title><link>https://mp.weixin.qq.com/s/-FQhijaGRxa0tUoTLMiPRg</link><description></description><content:encoded><![CDATA[只靠开源的时代已经过去，BAT都在这样做！]]></content:encoded><pubDate>Tue, 23 Mar 2021 19:15:07 +0800</pubDate></item><item><title>AAAI 2021最佳论文亚军：Attention+积分梯度=归因解释新方法</title><link>https://mp.weixin.qq.com/s/xnoLCOHPOI5O_iRM-73USg</link><description></description><content:encoded><![CDATA[AAAI 2021最佳论文亚军：Attention+积分梯度=归因解释新方法]]></content:encoded><pubDate>Tue, 23 Mar 2021 19:15:07 +0800</pubDate></item><item><title>岗位内推 | 阿里巴巴达摩院对话智能团队招聘自然语言处理实习生</title><link>https://mp.weixin.qq.com/s/rXkrg-dMFZv29ycH6m-vDw</link><description></description><content:encoded><![CDATA[岗位内推 | 阿里巴巴达摩院对话智能团队招聘自然语言处理实习生]]></content:encoded><pubDate>Tue, 23 Mar 2021 19:15:07 +0800</pubDate></item><item><title>2小时彻底搞懂指针与共用体！</title><link>https://mp.weixin.qq.com/s/Ezdvrz6Q50uDbhsbUkF4eg</link><description></description><content:encoded><![CDATA[2小时彻底搞懂指针与共用体！]]></content:encoded><pubDate>Mon, 22 Mar 2021 12:51:36 +0800</pubDate></item><item><title>口语语言理解（SLU）最新资源库：综述、数据集、开源论文</title><link>https://mp.weixin.qq.com/s/8Bd5zDSyhictHyQInE-FeQ</link><description></description><content:encoded><![CDATA[口语语言理解（SLU）最新资源库：综述、数据集、开源论文]]></content:encoded><pubDate>Mon, 22 Mar 2021 12:51:36 +0800</pubDate></item><item><title>直播 | AAAI 2021最佳论文：比Transformer更有效的长时间序列预测</title><link>https://mp.weixin.qq.com/s/u8vb2Su-y9a-j3NTBgyLSA</link><description></description><content:encoded><![CDATA[直播 | AAAI 2021最佳论文：比Transformer更有效的长时间序列预测]]></content:encoded><pubDate>Mon, 22 Mar 2021 12:51:36 +0800</pubDate></item><item><title>Transformer升级之路：Sinusoidal位置编码追根溯源</title><link>https://mp.weixin.qq.com/s/57iu8rPTXXG0jb2xxEVnTw</link><description></description><content:encoded><![CDATA[Transformer升级之路：Sinusoidal位置编码追根溯源]]></content:encoded><pubDate>Sun, 21 Mar 2021 13:47:03 +0800</pubDate></item><item><title>LSTM之父重提30年前的「快速权重存储系统」：线性Transformer只是它的一种变体</title><link>https://mp.weixin.qq.com/s/YRthkT3psXwsLL5JpurHVQ</link><description></description><content:encoded><![CDATA[LSTM之父重提30年前的「快速权重存储系统」：线性Transformer只是它的一种变体]]></content:encoded><pubDate>Sun, 21 Mar 2021 13:47:03 +0800</pubDate></item><item><title>推荐几个出论文的好方向！</title><link>https://mp.weixin.qq.com/s/XxzhwCkSWnaFi71CSNpUaw</link><description></description><content:encoded><![CDATA[推荐几个出论文的好方向！]]></content:encoded><pubDate>Sat, 20 Mar 2021 21:25:52 +0800</pubDate></item><item><title>如何使ResNet优于EfficientNet？改进训练方法和扩展策略就可以</title><link>https://mp.weixin.qq.com/s/35kiVU3ilPNFALkahJfVYg</link><description></description><content:encoded><![CDATA[如何使ResNet优于EfficientNet？改进训练方法和扩展策略就可以]]></content:encoded><pubDate>Sat, 20 Mar 2021 21:25:52 +0800</pubDate></item><item><title>线下沙龙报名 | 揭秘百度自动驾驶的核心技术</title><link>https://mp.weixin.qq.com/s/Yfw72lMjRU4aDm70ey2yDw</link><description></description><content:encoded><![CDATA[线下沙龙报名 | 揭秘百度自动驾驶的核心技术]]></content:encoded><pubDate>Fri, 19 Mar 2021 18:39:35 +0800</pubDate></item><item><title>CVPR 2021 | 基于跨任务场景结构知识迁移的单张深度图像超分辨率方法</title><link>https://mp.weixin.qq.com/s/sMyzuK3-Bip0EI9Wid7Ixg</link><description></description><content:encoded><![CDATA[CVPR 2021 | 基于跨任务场景结构知识迁移的单张深度图像超分辨率方法]]></content:encoded><pubDate>Fri, 19 Mar 2021 18:39:35 +0800</pubDate></item><item><title>内推 | 微软亚洲互联网工程院NLP组招聘AI算法工程师（社招 &amp; 校招）</title><link>https://mp.weixin.qq.com/s/6MeweZz7kIUYe2VILtIKNg</link><description></description><content:encoded><![CDATA[内推 | 微软亚洲互联网工程院NLP组招聘AI算法工程师（社招 & 校招）]]></content:encoded><pubDate>Fri, 19 Mar 2021 18:39:35 +0800</pubDate></item><item><title>做 NLP 算法研究，去大公司还是创业公司？</title><link>https://mp.weixin.qq.com/s/vTI1-aRGBh620XrwYBCHGg</link><description></description><content:encoded><![CDATA[做 NLP 算法研究，去大公司还是创业公司？]]></content:encoded><pubDate>Thu, 18 Mar 2021 13:44:33 +0800</pubDate></item><item><title>快手-中科大最新研究：利用对话式推荐解决用户冷启动问题</title><link>https://mp.weixin.qq.com/s/627wrUxkAPoRlO0YFxRcoA</link><description></description><content:encoded><![CDATA[快手-中科大最新研究：利用对话式推荐解决用户冷启动问题]]></content:encoded><pubDate>Thu, 18 Mar 2021 13:44:33 +0800</pubDate></item><item><title>以机器、深度学习和生理信号分类为关键词的学术论文整理</title><link>https://mp.weixin.qq.com/s/VcxNkue6bh34DmetqbNKMg</link><description></description><content:encoded><![CDATA[以机器、深度学习和生理信号分类为关键词的学术论文整理]]></content:encoded><pubDate>Thu, 18 Mar 2021 13:44:33 +0800</pubDate></item><item><title>商汤2021春季校园招聘正式启动！“春招百晓生”系列空宣强势来袭</title><link>https://mp.weixin.qq.com/s/FDGHG0i6K6StpTOCVGf1TA</link><description></description><content:encoded><![CDATA[商汤2021春季校园招聘正式启动！“春招百晓生”系列空宣强势来袭]]></content:encoded><pubDate>Wed, 17 Mar 2021 12:43:59 +0800</pubDate></item></channel></rss>