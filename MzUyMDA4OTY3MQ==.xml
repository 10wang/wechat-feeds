<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>浪尖聊大数据 | wechat-feeds</title><link>http://MzUyMDA4OTY3MQ.favicon.privacyhide.com/favicon.ico</link><description>主要分享大数据框架，如spark，flink，kafka，hbase原理源码，同时会分享数据仓库，图计算等浪尖擅长领域。</description><managingEditor> (hellodword)</managingEditor><pubDate>Fri, 12 Feb 2021 11:36:23 +0800</pubDate><image><url>http://MzUyMDA4OTY3MQ.favicon.privacyhide.com/favicon.ico</url><title>浪尖聊大数据 | wechat-feeds</title><link>http://MzUyMDA4OTY3MQ.favicon.privacyhide.com/favicon.ico</link><width>64</width><height>64</height></image><item><title>新年牛年吉祥，浪尖祝大家新年工作顺利，工资翻倍，牛年牛市，股票翻倍！</title><link>https://mp.weixin.qq.com/s/TfWGAe6JwD9NGj2ru3VBVw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXfm1uzE3S1pjY510Su8WXF2Dde8aeXsUUhIbhYGJAIrHxUibnVg8uAicHLnNteGoOibNvJH1oK7zdnicw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>新年牛年吉祥，浪尖祝大家新年工作顺利，工资翻倍，牛年牛市，股票翻倍！]]></content:encoded><pubDate>Fri, 12 Feb 2021 09:39:30 +0800</pubDate></item><item><title>Flink 助力美团数仓增量生产</title><link>https://mp.weixin.qq.com/s/SdKl-FBuHD6B1_iLX7QeFA</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXf1IWiaicCf76v6s4aJS3J05xrtdGMMKAEqa2WKVCPXiaAyVGlR9YLsXC0tu3EYicDShCa4rUibjSEouDw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>单日的高峰流量达 1.8 亿条每秒，美团数仓生产实践。]]></content:encoded><pubDate>Wed, 10 Feb 2021 22:19:45 +0800</pubDate></item><item><title>1.3 万亿条数据查询，如何做到毫秒级响应？</title><link>https://mp.weixin.qq.com/s/mVc1QnkH7zcLXnGqlAIKGw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXf1IWiaicCf76v6s4aJS3J05x0uiaKYYb3Uoe6q3NUtFnE8Ipgc8Vye13rVBPBPCj2UV9HIMXpQjfI1w/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>1.3 万亿条数据查询，如何做到毫秒级响应？]]></content:encoded><pubDate>Wed, 10 Feb 2021 22:19:45 +0800</pubDate></item><item><title>基于 Flink+Iceberg 构建企业级实时数据湖</title><link>https://mp.weixin.qq.com/s/rdrwyB2i_5h24trtqhZ9Nw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXfQ7pbTjaSQqfuqeUFASQ4GTccblibibhjfOibjiaHRsoBl5X9IbtiaAhxGBw5USuYXU9HeWP9dndeO4GQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>当 Apache Flink 遇见数据湖，会碰撞出怎样的火花？]]></content:encoded><pubDate>Tue, 09 Feb 2021 22:39:00 +0800</pubDate></item><item><title>Redis 的 8 大数据类型，写得非常好！</title><link>https://mp.weixin.qq.com/s/kyZXVFKqIBB1Lr244lgNWA</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXfQ7pbTjaSQqfuqeUFASQ4Ga93rzMV2ialCktibjZgEqJcSQ5c4HibBPNnICOZZR6bNpcZBQpkBwxY4w/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>redis数据类型]]></content:encoded><pubDate>Tue, 09 Feb 2021 22:39:00 +0800</pubDate></item><item><title>Flink实战 - Binlog日志并对接Kafka实战</title><link>https://mp.weixin.qq.com/s/QFlZ3ACvqi1Idp5FdB7EUg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXfrYVxlxd18jQ5USxErUgM8ltqFiaYpNW6TfvhYdAj7qOBEWIcoDElZxuI3Io3KiccZsr7XcS4dcY1A/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>Flink实战 - Binlog日志并对接Kafka实战]]></content:encoded><pubDate>Mon, 08 Feb 2021 22:31:30 +0800</pubDate></item><item><title>十道海量数据处理面试题</title><link>https://mp.weixin.qq.com/s/zLMXQKTDedH24Hd7tA8LYQ</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXfrYVxlxd18jQ5USxErUgM8ZicD6scC34XFlVviaUHcYEnoTicIaDIChdOgC6ewlia1ic8Fs37iaoGiaPMjA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>十道海量数据处理面试题]]></content:encoded><pubDate>Mon, 08 Feb 2021 22:31:30 +0800</pubDate></item><item><title>Flink 在实时金融数据湖的应用</title><link>https://mp.weixin.qq.com/s/VzCjNJK5EV8zm7WH6MFwag</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXfhHHDicffUD4pe7uT1CQdozkeeeVqWutbLAs6RYicyRCwfQiceOWvc9buvKVXMQaPVW3d00h5WQKcdw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>本文将从实时金融数据湖的建设背景、体系架构、场景实践三个方面分享。]]></content:encoded><pubDate>Sat, 06 Feb 2021 17:48:18 +0800</pubDate></item><item><title>5种经典的数据分析思维和方法</title><link>https://mp.weixin.qq.com/s/_jvXsP-UzThCUbthmsXsJg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXdtTiaAIjUu261pSprzGBIxMNRic4JA8DvrcIRWAdy65MRU9anEtB0PRZS1micFMgjeVcicx1R9q8BKNQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>这些方法你都知道吗]]></content:encoded><pubDate>Sat, 06 Feb 2021 17:48:18 +0800</pubDate></item><item><title>协同过滤推荐算法在MapReduce与Spark上实现对比</title><link>https://mp.weixin.qq.com/s/jtXmAeEQosE5rsWvLd8Cwg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXdtTiaAIjUu261pSprzGBIxMwVTUAZaS5tDKHpx6ApHFGzTCSaF3t30TzPLqFPlTO6l0qSC0YGBYHw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>本文将介绍基于物品的协同过滤推荐算法案例在TDW Spark与MapReudce上的实现对比，相比于MapReduce，TDW Spark执行时间减少了66%，计算成本降低了40%。]]></content:encoded><pubDate>Fri, 05 Feb 2021 12:24:22 +0800</pubDate></item><item><title>一篇文章，读懂Netty的高性能架构之道</title><link>https://mp.weixin.qq.com/s/Uq24FZWEmoT5X8HthrNx_Q</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXdtTiaAIjUu261pSprzGBIxMMOILdFSXCsNSDIll0LuWkiajXXGbZiaI3YfgibI34gKs56iaTNnNdt9njA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>一篇文章了解Netty，什么是Netty，Netty的优势，架构分析，IO模型，线程模型，链路有效性检测，流量整形，优雅停机！]]></content:encoded><pubDate>Fri, 05 Feb 2021 12:24:22 +0800</pubDate></item><item><title>数仓潮汐猎人 | 数据仓库企业数仓拉链表制作​</title><link>https://mp.weixin.qq.com/s/OEIQkbwYL_Q4-VR4PjKqzg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXcEdxicuwOvyC6NgdnVJ9sYy9XY2sWWCytzuPss3lH4CL6HXxVaCHruOcAsVMfvia2ibeCIIos24wfvQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>数仓潮汐猎人 | 数据仓库企业数仓拉链表制作​]]></content:encoded><pubDate>Thu, 04 Feb 2021 17:33:36 +0800</pubDate></item><item><title>用户建模教程：3步搭建一个流失模型</title><link>https://mp.weixin.qq.com/s/xJWHqDEivnwhP3q0nl5NCA</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXeg9eOZQyp91MX0UUUiaKxATWBnY2gsaGWy324bMvD4r7P3AaS9AtXSlY6o1mkmwb5Cd6sIiawYhhRA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>如题。]]></content:encoded><pubDate>Wed, 03 Feb 2021 11:51:26 +0800</pubDate></item><item><title>利用 Spark DataSource API 实现Rest数据源</title><link>https://mp.weixin.qq.com/s/ILxS5zCJak-WLkZtBIwBLw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXcqYC0bOBmJ3zNscDE6SqrOuIicqWJicsYL7YowfIAI8DAS1LHtE2WZbpuqvkg3Ao0PMF3VbnNQupmg/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>Spark DataSource API 的提出使得各个数据源按规范实现适配，那么就可以高效的利用Spark 的计算能力。本文则介绍如何利用Spark DataSource 对标准Rest接口实现读取...]]></content:encoded><pubDate>Mon, 01 Feb 2021 23:49:37 +0800</pubDate></item><item><title>浅谈 RESTful API</title><link>https://mp.weixin.qq.com/s/W0fkFm3TeMf-AuuncLq30w</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXcqYC0bOBmJ3zNscDE6SqrOXX8cuku3vGCibrcn0vhFByJvjkh8PKW8bwplOYudib9oRq8Izic6ictcOw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>如何正确地理解 RESTful ？]]></content:encoded><pubDate>Mon, 01 Feb 2021 23:49:37 +0800</pubDate></item><item><title>用户留存分析案例 | 以京东、淘宝、饿了么为例！</title><link>https://mp.weixin.qq.com/s/TeJtMB8iF8hlC7dwrTiJ5g</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXdploy4aZnt5AfSpP3OAxxlUiaKbZv6HBtoAJw6mZ7b6Lz0KxKqb7PWXnRuNQ4uSQlvKf1r7K2Q0RA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>用户留存应该怎么分析？]]></content:encoded><pubDate>Sun, 31 Jan 2021 15:02:24 +0800</pubDate></item><item><title>干货 | 优秀的数据敏感度应该如何培养？</title><link>https://mp.weixin.qq.com/s/vGqQZevCVuYEdH_7vwtSTg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/TwK74MzofXdploy4aZnt5AfSpP3OAxxlwdhxBicPibEUZVeEicRNjshSFdDOHhATkwiaRK4NlsNIk4zwEiaOptYPrEg/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>关于数据敏感度。]]></content:encoded><pubDate>Sun, 31 Jan 2021 15:02:24 +0800</pubDate></item></channel></rss>