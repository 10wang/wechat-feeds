<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>雨石记 | wechat-feeds</title><link>http://MzI4ODg3NDY2NQ.favicon.privacyhide.com/favicon.ico</link><description>记录一名Google工程师的技术成长之路，包括深度学习，架构，编程，见识等。</description><managingEditor> (hellodword)</managingEditor><pubDate>Wed, 14 Apr 2021 10:06:52 +0800</pubDate><image><url>http://MzI4ODg3NDY2NQ.favicon.privacyhide.com/favicon.ico</url><title>雨石记 | wechat-feeds</title><link>http://MzI4ODg3NDY2NQ.favicon.privacyhide.com/favicon.ico</link><width>64</width><height>64</height></image><item><title>PRADO: 见多了大模型，来聊一个200K的SOTA模型</title><link>https://mp.weixin.qq.com/s/xXaP7RC_GaBiaJ-6zG3AEg</link><description></description><content:encoded><![CDATA[PRADO: 见多了大模型，来聊一个200K的SOTA模型]]></content:encoded><pubDate>Tue, 13 Apr 2021 22:35:49 +0800</pubDate></item><item><title>Unit: 多模态多任务的统一Transformer模型</title><link>https://mp.weixin.qq.com/s/D8n8OI-aO0BBamfvTEVF9g</link><description></description><content:encoded><![CDATA[Unit: 多模态多任务的统一Transformer模型]]></content:encoded><pubDate>Mon, 12 Apr 2021 23:15:56 +0800</pubDate></item><item><title>GraphSAGE: 大规模图结构的归纳表示学习</title><link>https://mp.weixin.qq.com/s/kRrM0FJuXlBcE52GOLjnsQ</link><description></description><content:encoded><![CDATA[GraphSAGE: 大规模图结构的归纳表示学习]]></content:encoded><pubDate>Tue, 30 Mar 2021 00:05:56 +0800</pubDate></item><item><title>GPT-3: 告别梯度，大模型带来的红利</title><link>https://mp.weixin.qq.com/s/n7HOcY8VpkakCqesprxQLw</link><description></description><content:encoded><![CDATA[GPT-3: 告别梯度，大模型带来的红利]]></content:encoded><pubDate>Mon, 29 Mar 2021 00:41:46 +0800</pubDate></item><item><title>模型量化: 训练时噪音带来的极致压缩</title><link>https://mp.weixin.qq.com/s/J6FLnfxiRAgIRPSJZv1SXA</link><description></description><content:encoded><![CDATA[模型量化: 训练时噪音带来的极致压缩]]></content:encoded><pubDate>Mon, 15 Mar 2021 23:25:25 +0800</pubDate></item><item><title>深度学习模型的训练时内存次线性优化</title><link>https://mp.weixin.qq.com/s/RMDEvy-3-L-Rag1OrZLYhg</link><description></description><content:encoded><![CDATA[深度学习模型的训练时内存次线性优化]]></content:encoded><pubDate>Sun, 14 Mar 2021 16:01:30 +0800</pubDate></item><item><title>Gboard Smart Compose: 更广泛的智能拼写</title><link>https://mp.weixin.qq.com/s/cJ5FH3zQ7tCeYcoXSMYnNw</link><description></description><content:encoded><![CDATA[Gboard Smart Compose: 更广泛的智能拼写]]></content:encoded><pubDate>Wed, 03 Mar 2021 00:38:04 +0800</pubDate></item><item><title>模型量化: 只有整数计算的高效推理</title><link>https://mp.weixin.qq.com/s/du3hb2oM5X6bMocdOab4dg</link><description></description><content:encoded><![CDATA[模型量化: 只有整数计算的高效推理]]></content:encoded><pubDate>Sun, 28 Feb 2021 17:22:34 +0800</pubDate></item><item><title>Switch Transformer: 高效稀疏的万亿参数Transformer</title><link>https://mp.weixin.qq.com/s/XQSEg2_8_1lFqWdHVG6TVA</link><description></description><content:encoded><![CDATA[Switch Transformer: 高效稀疏的万亿参数Transformer]]></content:encoded><pubDate>Tue, 23 Feb 2021 23:21:05 +0800</pubDate></item><item><title>Pix2Pix-基于GAN的图像翻译</title><link>https://mp.weixin.qq.com/s/JFrHdm_9z3sfa7xrOak6tA</link><description></description><content:encoded><![CDATA[Pix2Pix-基于GAN的图像翻译]]></content:encoded><pubDate>Tue, 23 Feb 2021 23:21:05 +0800</pubDate></item><item><title>T5: 文本到文本的Transformer迁移学习</title><link>https://mp.weixin.qq.com/s/GU0ijr2Fk7m5uCZ04JDoEw</link><description></description><content:encoded><![CDATA[T5: 文本到文本的Transformer迁移学习]]></content:encoded><pubDate>Sun, 21 Feb 2021 15:16:00 +0800</pubDate></item></channel></rss>