<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>雨石记 | wechat-feeds</title><link>http://MzI4ODg3NDY2NQ.favicon.privacyhide.com/favicon.ico</link><description>记录一名Google工程师的技术成长之路，包括深度学习，架构，编程，见识等。</description><managingEditor> (hellodword)</managingEditor><pubDate>Wed, 19 May 2021 10:23:19 +0800</pubDate><image><url>http://MzI4ODg3NDY2NQ.favicon.privacyhide.com/favicon.ico</url><title>雨石记 | wechat-feeds</title><link>http://MzI4ODg3NDY2NQ.favicon.privacyhide.com/favicon.ico</link><width>64</width><height>64</height></image><item><title>Adapter: 高效NLP迁移学习</title><link>https://mp.weixin.qq.com/s/Z3-xnQLZqUL38rDRIsnFhw</link><description></description><content:encoded><![CDATA[Adapter: 高效NLP迁移学习]]></content:encoded><pubDate>Wed, 19 May 2021 08:09:44 +0800</pubDate></item><item><title>DCCL: 推荐系统的端+云联动学习</title><link>https://mp.weixin.qq.com/s/0FIYCGtUmW0sKcHu40gZKw</link><description></description><content:encoded><![CDATA[DCCL: 推荐系统的端+云联动学习]]></content:encoded><pubDate>Mon, 17 May 2021 22:33:34 +0800</pubDate></item><item><title>Vit: 图像识别上的Transformer</title><link>https://mp.weixin.qq.com/s/EPLQ2leRAnpSKjF4QwOpSw</link><description></description><content:encoded><![CDATA[Vit: 图像识别上的Transformer]]></content:encoded><pubDate>Mon, 17 May 2021 07:32:03 +0800</pubDate></item><item><title>SIM: 基于搜索的超长行为序列上的用户兴趣建模</title><link>https://mp.weixin.qq.com/s/tVnLMJh20Oshovd7BfMXzA</link><description></description><content:encoded><![CDATA[SIM: 基于搜索的超长行为序列上的用户兴趣建模]]></content:encoded><pubDate>Sun, 09 May 2021 21:35:06 +0800</pubDate></item><item><title>EdgeRec: 手机淘宝的端上推荐</title><link>https://mp.weixin.qq.com/s/3B7SCqK1CLDAzKTZeIhShQ</link><description></description><content:encoded><![CDATA[EdgeRec: 手机淘宝的端上推荐]]></content:encoded><pubDate>Fri, 07 May 2021 08:54:24 +0800</pubDate></item><item><title>PRADO: 见多了大模型，来聊一个200K的SOTA模型</title><link>https://mp.weixin.qq.com/s/xXaP7RC_GaBiaJ-6zG3AEg</link><description></description><content:encoded><![CDATA[PRADO: 见多了大模型，来聊一个200K的SOTA模型]]></content:encoded><pubDate>Tue, 13 Apr 2021 22:35:49 +0800</pubDate></item><item><title>Unit: 多模态多任务的统一Transformer模型</title><link>https://mp.weixin.qq.com/s/D8n8OI-aO0BBamfvTEVF9g</link><description></description><content:encoded><![CDATA[Unit: 多模态多任务的统一Transformer模型]]></content:encoded><pubDate>Mon, 12 Apr 2021 23:15:56 +0800</pubDate></item><item><title>GraphSAGE: 大规模图结构的归纳表示学习</title><link>https://mp.weixin.qq.com/s/kRrM0FJuXlBcE52GOLjnsQ</link><description></description><content:encoded><![CDATA[GraphSAGE: 大规模图结构的归纳表示学习]]></content:encoded><pubDate>Tue, 30 Mar 2021 00:05:56 +0800</pubDate></item><item><title>GPT-3: 告别梯度，大模型带来的红利</title><link>https://mp.weixin.qq.com/s/n7HOcY8VpkakCqesprxQLw</link><description></description><content:encoded><![CDATA[GPT-3: 告别梯度，大模型带来的红利]]></content:encoded><pubDate>Mon, 29 Mar 2021 00:41:46 +0800</pubDate></item><item><title>模型量化: 训练时噪音带来的极致压缩</title><link>https://mp.weixin.qq.com/s/J6FLnfxiRAgIRPSJZv1SXA</link><description></description><content:encoded><![CDATA[模型量化: 训练时噪音带来的极致压缩]]></content:encoded><pubDate>Mon, 15 Mar 2021 23:25:25 +0800</pubDate></item></channel></rss>