<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>机器学习研究组订阅 | wechat-feeds</title><link>http://MzU1NTUxNTM0Mg.favicon.privacyhide.com/favicon.ico</link><description>机器学习研究会由百度七剑客雷鸣先生创办，旨在推动AI的技术发展和产业落地。参与组织北大、清华”AI前沿与产业趋势“公开课，广泛的和高校、企业、创业、VC开展合作，自身也参与优秀AI项目的投资和孵化。</description><managingEditor> (hellodword)</managingEditor><pubDate>Tue, 26 Jan 2021 20:51:50 +0800</pubDate><image><url>http://MzU1NTUxNTM0Mg.favicon.privacyhide.com/favicon.ico</url><title>机器学习研究组订阅 | wechat-feeds</title><link>http://MzU1NTUxNTM0Mg.favicon.privacyhide.com/favicon.ico</link><width>64</width><height>64</height></image><item><title>堪比当年的LSTM，Transformer引燃机器学习圈：它是万能的</title><link>https://mp.weixin.qq.com/s/lMOT6kOjvc72vcf_Z1PmzQ</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkbQG08nBN3fCJ3lImNict75GTlcaOWJt3e6K37ZT1hTSXOtrfZUdSOKXlMgoW3Dy1DQ6EenoXmBBZQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>谷歌研究科学家 David Ha：Transformer 是新的 LSTM。2017 年 6 月谷歌发布论文]]></content:encoded><pubDate>Tue, 26 Jan 2021 18:47:48 +0800</pubDate></item><item><title>谷歌Waymo CEO：特斯拉「根本不是竞争对手」，马斯克表震惊！</title><link>https://mp.weixin.qq.com/s/m1ROjsf70lmnNHlXsqHh9g</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkbQG08nBN3fCJ3lImNict75G9m1btMnu8TKbbE5qddIiaDcSdttxVoI3BYYZUQlsgflSj1ZsArM97wQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>马斯克的特斯拉最近竟然遭遇「藐视」：根本不是竞争对手！ 如此大放厥词的是美国另一家自动驾驶汽车公司Waymo]]></content:encoded><pubDate>Tue, 26 Jan 2021 18:47:48 +0800</pubDate></item><item><title>训练一个 130 亿参数的模型要用几个 GPU？微软：一个就够</title><link>https://mp.weixin.qq.com/s/mJH4eIoV7m8cYpRSyT4xow</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkbQG08nBN3fCJ3lImNict75GKkbqAaDuibmMLyzXEBPHLXTbAQY5ibibYicBpOuibkp26icwanOgpEicxdtKA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>现在的模型动辄数百、数千亿参数，普通人训不动怎么办？前不久，谷歌发布了参数量为 1.6 万亿的语言模型 Sw]]></content:encoded><pubDate>Tue, 26 Jan 2021 18:47:48 +0800</pubDate></item><item><title>从零开始，用Python徒手写线性回归</title><link>https://mp.weixin.qq.com/s/caLVc8ob1FUvUSnRJuyz1g</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3Tka0VmTAo1A8ErvKY7iaFNHveicoYjp62MWrnArtaicCic4q84ibTkN6vQ8laeeMicy6BtYQ9lHe0n1zLyvg/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>先放下 Scikit-learn，我们来看一看真正的技术。对于大多数数据科学家而言，线性回归方法是他们进行统]]></content:encoded><pubDate>Mon, 25 Jan 2021 21:18:18 +0800</pubDate></item><item><title>因果推理、正则化上榜：权威专家盘点过去50年最重要的统计学思想</title><link>https://mp.weixin.qq.com/s/eEcq-VvTq6-RhvDv9vFtGg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3Tka0VmTAo1A8ErvKY7iaFNHveLm6nz3ZfaSl6C7ROZeCT4IPNNcVQj2HVxlWEut87u9ZB6Nyfr8P4wQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>近日，图灵奖得主、贝叶斯网络之父 Judea Pearl 点赞了一篇论文，这篇论文总结了过去 50 年出现的]]></content:encoded><pubDate>Mon, 25 Jan 2021 21:18:18 +0800</pubDate></item><item><title>ICLR 2021｜自解释神经网络，直接写入了特征的重要值：Shapley Explanation Networks</title><link>https://mp.weixin.qq.com/s/24ojYBBY9Yi0PpADIqZfAA</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3Tka0VmTAo1A8ErvKY7iaFNHve6jmYic9aRs108EC3KQibib1nDaMibBr41WNMWMmXiaVveHOibf3EXzjC3wNw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>TL;DR：我们将特征的重要值直接写进神经网络，作为层间特征，这样的神经网络模型有了新的功能：1. 层间特征]]></content:encoded><pubDate>Mon, 25 Jan 2021 21:18:18 +0800</pubDate></item><item><title>TPAMI 2021 | 换个损失函数就能实现数据扩增？</title><link>https://mp.weixin.qq.com/s/hVEaUwnffymuHx0Z1HUk0A</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkaEwuIReOCtd3XjJ70S2okCAm6B4J5RsnicZxp9FibQISuP0aRTIKoeOngmT7Wibh5MJoyzawsLCwruw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>作者 | 王语霖AI科技评论今天介绍一篇刚刚被IEEE Transactions on Pattern An]]></content:encoded><pubDate>Sat, 23 Jan 2021 17:42:44 +0800</pubDate></item><item><title>最佳论文！商汤提出手机端实时单目三维重建系统 | ISMAR 2020</title><link>https://mp.weixin.qq.com/s/9Lh1E0BPd_AxRsOHuaHONw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkaEwuIReOCtd3XjJ70S2okCrYQddTBq638XSFLqG6tl194DQABbN66kMkm127a1Ayv8s6VJXNakRQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>摘要 · 看点商汤研究院和浙江大学 CAD&CG 国家重点实验室合作研发了一个手机端实时单目三维重建系统 M]]></content:encoded><pubDate>Sat, 23 Jan 2021 17:42:44 +0800</pubDate></item><item><title>PyTorch深度学习模型训练加速指南2021</title><link>https://mp.weixin.qq.com/s/vllBptSCXefOXpW8bIfm1w</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkaEwuIReOCtd3XjJ70S2okCRlZSickHPlBsFJqRDQ9sW9jGDQstn2RMuibEdaAC9E0q7nYKvkg48iaeg/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>作者 | LORENZ KUHN   编译 | ronghuaiyang导读简要介绍在PyTorch中加速深]]></content:encoded><pubDate>Sat, 23 Jan 2021 17:42:44 +0800</pubDate></item><item><title>谁是全球最顶级AI实验室？DeepMind、OpenAI和FAIR霸榜前三</title><link>https://mp.weixin.qq.com/s/NPF2XWi8Uj_YSC2ifLiEPQ</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3Tka4xOe11Dlh5LEElHMt9LmahdgHbW5dvs5IHRChuL0QqljbjvsGptryfb5apm5qllAzjNY2E8V4sw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>美国的大型科技公司如谷歌、 Facebook、亚马逊、苹果和微软，在过去十年里都建立了专门的人工智能实验室。]]></content:encoded><pubDate>Fri, 22 Jan 2021 18:58:45 +0800</pubDate></item><item><title>因果推理、正则化上榜：权威专家盘点过去50年最重要的统计学思想</title><link>https://mp.weixin.qq.com/s/BY5bWOTn-LJ6Qb2CAxOwew</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3Tka4xOe11Dlh5LEElHMt9LmateKDp0SdDfZsYRYyZxXa0udtSM6XV829lVGQ1UZM22R4USPQUdAMdA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>近日，图灵奖得主、贝叶斯网络之父 Judea Pearl 点赞了一篇论文，这篇论文总结了过去 50 年出现的]]></content:encoded><pubDate>Fri, 22 Jan 2021 18:58:45 +0800</pubDate></item><item><title>ICLR 2021｜自解释神经网络，直接写入了特征的重要值：Shapley Explanation Networks</title><link>https://mp.weixin.qq.com/s/LIWgAW_XcbnuHsPVB0nbIg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3Tka4xOe11Dlh5LEElHMt9Lmaa6YD9u8EUHAcIEKNSM33XhY1zOkqWg9HJVvRFz6rXBBT9NWPNqvYWw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>TL;DR：我们将特征的重要值直接写进神经网络，作为层间特征，这样的神经网络模型有了新的功能：1. 层间特征]]></content:encoded><pubDate>Fri, 22 Jan 2021 18:58:45 +0800</pubDate></item><item><title>寒武纪首颗AI训练芯片亮相：7纳米制程，算力提升四倍，已规模化出货</title><link>https://mp.weixin.qq.com/s/SCvSXSY0cr6yaUYnljgLhg</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkanN3u2IY5BMX2RK00OoFWHBKVx86B2HpSPdxra3x5ib7zZbibibKaqwhPBOpFfSiaGeibUq3eckYz0PQQ/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>1 月 21 日，寒武纪思元 290 智能芯片及加速卡、玄思 1000 智能加速器在官网低调亮相，寒武纪表示]]></content:encoded><pubDate>Thu, 21 Jan 2021 18:14:20 +0800</pubDate></item><item><title>Nature盘点：从Fortran、arXiv到AlexNet，这些代码改变了科学界</title><link>https://mp.weixin.qq.com/s/3z8kp7JMSafR90hWiIFFGQ</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkanN3u2IY5BMX2RK00OoFWH5ibO729FZfp52EU056y6PCTQwhLYiaEYUI1ZAc9IQVXpicibibvfL8iadqicw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>从 Fortran 编译器到 arXiv 预印本库、AlexNet，这些计算机代码和平台改变了科学界。201]]></content:encoded><pubDate>Thu, 21 Jan 2021 18:14:20 +0800</pubDate></item><item><title>全连接层的作用是什么？</title><link>https://mp.weixin.qq.com/s/dr4ctj10YUiARUT4QknoxA</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkanN3u2IY5BMX2RK00OoFWHTvSjMHQmDVDz07Zbs6Wuuww4DWdcqmrzpua9dRRuaJTSAA8gu57MZw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>来自 | 知乎作者 | 魏秀参地址 | https://www.zhihu.com/question/410]]></content:encoded><pubDate>Thu, 21 Jan 2021 18:14:20 +0800</pubDate></item><item><title>2020年这10大ML、NLP研究最具影响力：为什么？接下来如何发展？</title><link>https://mp.weixin.qq.com/s/ztxAr0SdmGcY_05B33CTJw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkYibibCfIibvxRic2du6TROia3EaEs7PMvibpeFYYgepOB8ia6gGibyrro1JeIh6xkkuMBC2tfKOSAEqB9NXA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>去年有哪些机器学习重要进展是你必须关注的？听听 DeepMind 研究科学家怎么说。2020 年因为新冠疫情]]></content:encoded><pubDate>Wed, 20 Jan 2021 18:49:20 +0800</pubDate></item><item><title>权值衰减和 L2 正则化傻傻分不清楚？</title><link>https://mp.weixin.qq.com/s/15vUOBq2t_ZbtWCSFWOoWQ</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkYibibCfIibvxRic2du6TROia3EaMr0lCfIoqVYfQpUH5Lz0qlUPl6IBtwPNNHIicwZFTMawiam9uqJMLibSw/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>作者 | Divyanshu Mishra 编译 | ronghuaiyang导读权值衰减和L2正则化，到底]]></content:encoded><pubDate>Wed, 20 Jan 2021 18:49:20 +0800</pubDate></item><item><title>A survey on Image Data Augmentation 数据增强文献综述</title><link>https://mp.weixin.qq.com/s/niIaYYR6s9xBjFJF_J7HHw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkYibibCfIibvxRic2du6TROia3EaicMjXPY7cTqKlozHibb2SYrd9Q8voupzQu2EWGrWdAbNZLBbWLEIj8LA/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>来自 | 知乎作者 | chaser地址 | https://zhuanlan.zhihu.com/p/76]]></content:encoded><pubDate>Wed, 20 Jan 2021 18:49:20 +0800</pubDate></item><item><title>ImageNet的top-1终于上了90%，网友质疑：用额外数据集还不公开，让人怎么信服？</title><link>https://mp.weixin.qq.com/s/RRrU7-nootQyI2Kl3UZ1nw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkYly5PuRZ5V6nUrVuBTYssvUvtfnj1C8UfEG2u1E4jQCoG9N9ib735O07nb0z7diaochqHHpzM2Tf3A/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>Quoc Le：我原本以为 ImageNet 的 top-1 准确率 85% 就到头了，现在看来，这个上限难]]></content:encoded><pubDate>Tue, 19 Jan 2021 17:44:00 +0800</pubDate></item><item><title>TPAMI2021|深度学习行人重识别综述与展望</title><link>https://mp.weixin.qq.com/s/NgDUrxKSQLgr252IbNSDAw</link><description></description><content:encoded><![CDATA[<img src="http://mmbiz.qpic.cn/mmbiz_jpg/ibAQnkbS3TkYly5PuRZ5V6nUrVuBTYssvSZicWSeP55dG3hP4YKVibdwgCsbskqlItpLGvUjOgmhJNYYKW7ZSiaGcg/0?wx_fmt=jpeg" referrerpolicy="no-referrer"><br>来自 | 知乎作者 | 叶茫地址 | https://zhuanlan.zhihu.com/p/342249]]></content:encoded><pubDate>Tue, 19 Jan 2021 17:44:00 +0800</pubDate></item></channel></rss>